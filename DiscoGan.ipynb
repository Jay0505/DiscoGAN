{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "DiscoGan.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFLg6jDAjd_Y",
        "colab_type": "code",
        "outputId": "77c75f5c-4685-4f51-c340-f0592fbb3f0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDbQwYXKkV-u",
        "colab_type": "code",
        "outputId": "e8c5fa0b-067f-4579-c858-755e31da8b8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qQ8cCzekWj-",
        "colab_type": "code",
        "outputId": "e076d332-653c-4cfb-d1ec-c7afe50f9a9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!ls \"/content/drive/My Drive\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Archive 2.zip'       CycleGAN.ipynb\t     horse2zebra   results.csv\n",
            " Assignment_2.ipynb   DiscoGan.ipynb\t     Image_1\n",
            "'Colab Notebooks'    'Getting started.pdf'   Image_2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uU1BIPwkiZl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbJ1CfZ-kWrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvWY3DIWjd_h",
        "colab_type": "text"
      },
      "source": [
        "#########################################################################################################\n",
        "######################################## HELPER FUNCTIONS ###############################################\n",
        "#########################################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sy6Petojd_m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class helper_functions:\n",
        "    \n",
        "    \n",
        "    def conv_ops_of_Gen_and_Disc(self, data, num_filters, filter_size, padding,activation_fn, wts_initializer,\n",
        "                                    bias_initializer, normalizer_fn, batch_size, is_Gen = True):\n",
        "        \n",
        "        stride = 2\n",
        "        if is_Gen:\n",
        "            last_stride = 1\n",
        "            last_activation = tf.nn.relu\n",
        "        else:\n",
        "            last_stride = 2\n",
        "            last_activation = activation_fn\n",
        "            \n",
        "            \n",
        "        conv_1 = tf.contrib.layers.conv2d(inputs = data, num_outputs = num_filters[0], \n",
        "                                             kernel_size = filter_size, stride = stride,\n",
        "                                             padding = padding, activation_fn = activation_fn,\n",
        "                                             weights_initializer = wts_initializer, scope = 'conv_1', \n",
        "                                             biases_initializer = bias_initializer)\n",
        "        \n",
        "        conv_2 = tf.contrib.layers.conv2d(inputs = conv_1, num_outputs = num_filters[1], \n",
        "                                             kernel_size = filter_size, stride = stride,\n",
        "                                             padding = padding, activation_fn = activation_fn,\n",
        "                                             weights_initializer = wts_initializer, scope = 'conv_2', \n",
        "                                             biases_initializer = bias_initializer, normalizer_fn = normalizer_fn)\n",
        "        \n",
        "        conv_3 = tf.contrib.layers.conv2d(inputs = conv_2, num_outputs = num_filters[2], \n",
        "                                             kernel_size = filter_size, stride = stride,\n",
        "                                             padding = padding, activation_fn = activation_fn,\n",
        "                                             weights_initializer = wts_initializer, scope = 'conv_3', \n",
        "                                             biases_initializer = bias_initializer, normalizer_fn = normalizer_fn)\n",
        "        \n",
        "        \n",
        "        conv_4 = tf.contrib.layers.conv2d(inputs = conv_3, num_outputs = num_filters[3], \n",
        "                                             kernel_size = filter_size, stride = stride,\n",
        "                                             padding = padding, activation_fn = activation_fn,\n",
        "                                             weights_initializer = wts_initializer, scope = 'conv_4', \n",
        "                                             biases_initializer = bias_initializer, normalizer_fn = normalizer_fn)\n",
        "        \n",
        "        conv_5 = tf.contrib.layers.conv2d(inputs = conv_4, num_outputs = num_filters[4], \n",
        "                                             kernel_size = filter_size, stride = last_stride,\n",
        "                                             padding = padding, activation_fn = last_activation,\n",
        "                                             weights_initializer = wts_initializer, scope = 'conv_5', \n",
        "                                             biases_initializer = bias_initializer, normalizer_fn = normalizer_fn)\n",
        "        \n",
        "        if is_Gen:\n",
        "            return tf.reshape(conv_5, [-1, 8, 8, 128])\n",
        "        \n",
        "        \n",
        "        else:\n",
        "            conv_5_flatten = tf.reshape(conv_5, [-1, 2 * 2 * 512])\n",
        "            fc_1 = tf.contrib.layers.fully_connected(inputs = conv_5_flatten, num_outputs = 512, \n",
        "                                                     activation_fn = activation_fn, \n",
        "                                                     weights_initializer = wts_initializer, scope = \"fc_1\", \n",
        "                                                     normalizer_fn = normalizer_fn)\n",
        "            fc_2 = tf.contrib.layers.fully_connected(inputs = fc_1, num_outputs = 1, \n",
        "                                                     activation_fn = tf.nn.sigmoid, \n",
        "                                                     weights_initializer = wts_initializer, scope = \"fc_2\", \n",
        "                                                     normalizer_fn = normalizer_fn)\n",
        "            return fc_2\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRDaLhNIjd_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHGVf_8cjd_u",
        "colab_type": "text"
      },
      "source": [
        "#########################################################################################################\n",
        "######################################## GENERATOR ######################################################\n",
        "#########################################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzMIUc55jd_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class Generator:\n",
        "    def __init__(self, name, helper_functions, batch_size):\n",
        "        self.gen_name         = name\n",
        "        self.filter_size      = 4\n",
        "        self.enc_num_filters  = [32, 64, 128, 256, 512]\n",
        "        self.dec_num_filters  = [256, 128, 64, 3]\n",
        "        self.enc_stride       = 2\n",
        "        self.dec_stride       = 1\n",
        "        self.padding          = 'SAME'\n",
        "        self.enc_activation   = tf.nn.leaky_relu\n",
        "        self.dec_activation   = tf.nn.relu\n",
        "        self.wts_initializer  = tf.truncated_normal_initializer(stddev=0.02)\n",
        "        self.bias_initializer = tf.zeros_initializer()\n",
        "        self.normalizer_fn    = tf.contrib.layers.batch_norm\n",
        "        self.helper           = helper_functions\n",
        "        self.batch_size       = batch_size\n",
        "        \n",
        "        \n",
        "    #################################### Encoding part #################################################\n",
        "    \n",
        "    def Gen_encoder(self, data):\n",
        "        \n",
        "        return self.helper.conv_ops_of_Gen_and_Disc(data, self.enc_num_filters, self.filter_size, self.padding, \n",
        "                                                    self.enc_activation, self.wts_initializer, self.bias_initializer, \n",
        "                                                    self.normalizer_fn, self.batch_size, True)\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    #################################### Decoding part #################################################\n",
        "    \n",
        "    def Gen_decoder(self, data):\n",
        "\n",
        "        dec_conv_1 = tf.contrib.layers.conv2d(inputs = data, num_outputs = self.dec_num_filters[0], \n",
        "                                             kernel_size = self.filter_size, stride = self.dec_stride,\n",
        "                                             padding = self.padding, activation_fn = self.dec_activation,\n",
        "                                             weights_initializer = self.wts_initializer, scope = 'dec_conv_1', \n",
        "                                             biases_initializer = self.bias_initializer, normalizer_fn = self.normalizer_fn)\n",
        "        dec_conv_1 = tf.reshape(dec_conv_1, [-1, 16, 16, 64])\n",
        "  \n",
        "\n",
        "        #####\n",
        "        dec_conv_2 = tf.contrib.layers.conv2d(inputs = dec_conv_1, num_outputs = self.dec_num_filters[1], \n",
        "                                             kernel_size = self.filter_size, stride = self.dec_stride,\n",
        "                                             padding = self.padding, activation_fn = self.dec_activation,\n",
        "                                             weights_initializer = self.wts_initializer, scope = 'dec_conv_2', \n",
        "                                             biases_initializer = self.bias_initializer, normalizer_fn = self.normalizer_fn)\n",
        "        dec_conv_2 = tf.reshape(dec_conv_2, [-1, 32, 32, 32])\n",
        "        \n",
        "\n",
        "        #####\n",
        "        dec_conv_3 = tf.contrib.layers.conv2d(inputs = dec_conv_2, num_outputs = self.dec_num_filters[2], \n",
        "                                             kernel_size = self.filter_size, stride = self.dec_stride,\n",
        "                                             padding = self.padding, activation_fn = self.dec_activation,\n",
        "                                             weights_initializer = self.wts_initializer, scope = 'dec_conv_3', \n",
        "                                             biases_initializer = self.bias_initializer, normalizer_fn = self.normalizer_fn)\n",
        "        dec_conv_3 = tf.reshape(dec_conv_3, [-1, 64, 64, 16])\n",
        "        \n",
        "\n",
        "        #####\n",
        "        dec_conv_4 = tf.contrib.layers.conv2d(inputs = dec_conv_3, num_outputs = self.dec_num_filters[3], \n",
        "                                             kernel_size = self.filter_size, stride = self.dec_stride,\n",
        "                                             padding = self.padding, activation_fn = self.dec_activation,\n",
        "                                             weights_initializer = self.wts_initializer, scope = 'dec_conv_4', \n",
        "                                             biases_initializer = self.bias_initializer, normalizer_fn = self.normalizer_fn)\n",
        "        \n",
        "        return dec_conv_4\n",
        "    \n",
        "    \n",
        "    \n",
        "    #################################### Feed forward #################################################\n",
        "    \n",
        "    def gen_feed_forward(self, data):\n",
        "        with tf.variable_scope(self.gen_name, reuse = tf.AUTO_REUSE):\n",
        "            encoding_res = self.Gen_encoder(data)\n",
        "            decoding_res = self.Gen_decoder(encoding_res)\n",
        "            \n",
        "            self.gen_variables     = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, self.gen_name)\n",
        "            return decoding_res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BzolTk1jd_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdC2b8ezjd_4",
        "colab_type": "text"
      },
      "source": [
        "#########################################################################################################\n",
        "######################################## DISCRIMINATOR ##################################################\n",
        "#########################################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOOsquuxjd_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator:\n",
        "    def __init__(self, name, helper_functions, batch_size):\n",
        "        self.disc_name        = name\n",
        "        self.filter_size      = 4\n",
        "        self.num_filters      = [32, 64, 128, 256, 512]\n",
        "        self.activation_fn    = tf.nn.leaky_relu\n",
        "        self.wts_initializer  = tf.truncated_normal_initializer(stddev=0.02)\n",
        "        self.bias_initializer = tf.zeros_initializer()\n",
        "        self.normalizer_fn    = tf.contrib.layers.batch_norm\n",
        "        self.helper           = helper_functions\n",
        "        self.padding          = 'SAME'\n",
        "        self.batch_size       = batch_size\n",
        "        \n",
        "    #################################### Feed forward #################################################\n",
        "    \n",
        "    def disc_feed_forward(self, data):\n",
        "        with tf.variable_scope(self.disc_name, reuse = tf.AUTO_REUSE):\n",
        "            disc_res = self.helper.conv_ops_of_Gen_and_Disc(data, self.num_filters, self.filter_size, self.padding, \n",
        "                                                    self.activation_fn, self.wts_initializer, self.bias_initializer, \n",
        "                                                    self.normalizer_fn, self.batch_size, False)\n",
        "    \n",
        "            self.disc_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, self.disc_name)\n",
        "            return disc_res\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJ8J4uS7jd_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IE1LWvwIjd__",
        "colab_type": "text"
      },
      "source": [
        "#########################################################################################################\n",
        "######################################## DATA PROCESSING ################################################\n",
        "#########################################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k51iN6sjeAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Data_reading_and_processing:\n",
        "    def __init__(self, data_X_dir, data_Y_dir, val_X_dir, val_Y_dir):\n",
        "\n",
        "        self.new_data_X_dir       = data_X_dir # '/Users/vijay/Downloads/new_bags/train/'\n",
        "        self.new_data_Y_dir       = data_Y_dir #'/Users/vijay/Downloads/new_shoes/train/'\n",
        "        self.new_val_X_dir        = val_X_dir  # '/Users/vijay/Downloads/new_bags/val/'\n",
        "        self.new_val_Y_dir        = val_Y_dir  # '/Users/vijay/Downloads/new_shoes/val/'\n",
        "        self.train_X_images_names = os.listdir(self.new_data_X_dir)\n",
        "        self.train_Y_images_names = os.listdir(self.new_data_Y_dir)\n",
        "        self.val_X_images_names   = os.listdir(self.new_val_X_dir)\n",
        "        self.val_Y_images_names   = os.listdir(self.new_val_Y_dir)\n",
        "        \n",
        "\n",
        "        \n",
        "    \n",
        "    ########################################################################\n",
        "    \n",
        "    def crop_and_save_images_into_new_dir(self, prev_path, new_path, images_names):\n",
        "    \n",
        "        for image_name in images_names:\n",
        "            image = Image.open(prev_path + image_name)\n",
        "            image = image.resize([128,64])\n",
        "            image = image.crop([64,0,128,64])\n",
        "            image.save(new_path + image_name)\n",
        "\n",
        "        \n",
        "        \n",
        "    ########################################################################\n",
        "    \n",
        "    def get_batch(self, index, images_names, path, batch_size):\n",
        "\n",
        "        images_arr = []\n",
        "        images_batch = images_names[index : index + batch_size]\n",
        "        for image_name in images_batch:\n",
        "            \n",
        "            image = cv2.resize(cv2.imread(path + image_name), (64, 64))\n",
        "#             img = cv2.normalize(image, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
        "            img = cv2.normalize(image, None, 0, 128, cv2.NORM_MINMAX)\n",
        "            images_arr.append(img)\n",
        "        return np.asarray(images_arr)\n",
        "    \n",
        "    \n",
        "    ##########################################################################\n",
        "    # only for testing\n",
        "    def get_input_batch(self, index, images_names, path, batch_size):\n",
        "        images_arr = []\n",
        "        images_batch = images_names[index : index + batch_size]\n",
        "        for image_name in images_batch:\n",
        "            \n",
        "            image = cv2.resize(cv2.imread(path + image_name), (128, 64))\n",
        "            img = cv2.normalize(image, None, 0, 128, cv2.NORM_MINMAX)\n",
        "            img = img[0 : 64, 64 : 128]\n",
        "            images_arr.append(img)\n",
        "\n",
        "        return np.asarray(images_arr)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1SonJ4fjeAF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rArrJYVWjeAJ",
        "colab_type": "text"
      },
      "source": [
        "#########################################################################################################\n",
        "########################################   DISCO GAN   ##################################################\n",
        "#########################################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYQwDoE3jeAK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DISCO_GAN:\n",
        "    def __init__(self, image_shape, batch_size, learning_rate, num_epochs, to_restore, \n",
        "                                                   helper_functions, output_dir, data_object):\n",
        "        self.image_shape = image_shape\n",
        "        self.batch_size  = batch_size\n",
        "        self.lr          = learning_rate\n",
        "\n",
        "        self.num_epochs  = num_epochs\n",
        "        self.to_restore  = to_restore\n",
        "        self.helper      = helper_functions\n",
        "        self.output_dir  = output_dir  # '/Users/vijay/Downloads/DiscoGAN/test/'\n",
        "        self.X           = tf.placeholder(tf.float32, [None, self.image_shape[0], self.image_shape[1], self.image_shape[2]])\n",
        "        self.Y           = tf.placeholder(tf.float32, [None, self.image_shape[0], self.image_shape[1], self.image_shape[2]])\n",
        "        self.data_object = data_object\n",
        "        \n",
        "        self.check_point_dir = self.output_dir + 'checkpoints/'\n",
        "        self.global_step     = tf.Variable(0, name = 'global_step', trainable = False)\n",
        "\n",
        "    ########################################################################\n",
        "    \n",
        "    def get_optimizer(self):\n",
        "        \n",
        "        with tf.variable_scope('optim', reuse = tf.AUTO_REUSE):\n",
        "            \n",
        "            Gen_XY = Generator('Gen_XY', self.helper, self.batch_size) \n",
        "            Gen_YX = Generator('Gen_YX', self.helper, self.batch_size) \n",
        "            \n",
        "            Disc_X = Discriminator('Disc_X', self.helper, self.batch_size)\n",
        "            Disc_Y = Discriminator('Disc_Y', self.helper, self.batch_size)\n",
        "            \n",
        "            '''\n",
        "            naming conventions:\n",
        "                    gen_source_(result)\n",
        "                    \n",
        "                    gen_x_fake_y\n",
        "                    \n",
        "                        source = x\n",
        "                        result = fake_y\n",
        "                    \n",
        "                    gen_y_fake_x \n",
        "                    \n",
        "                        source = y\n",
        "                        result = fake_x\n",
        "                    \n",
        "                    gen_fake_y_fake_x\n",
        "                    \n",
        "                        source = fake_y\n",
        "                        result = fake_x\n",
        "                    \n",
        "                    gen_fake_x_fake_y\n",
        "                    \n",
        "                        source = fake_x\n",
        "                        result = fake_y\n",
        "            '''\n",
        "            self.gen_x_fake_y = Gen_XY.gen_feed_forward(self.X) # constructs fake_Y images from X\n",
        "            self.gen_y_fake_x = Gen_YX.gen_feed_forward(self.Y) # constructs fake_X images from Y\n",
        "            \n",
        "            gen_fake_y_fake_x = Gen_YX.gen_feed_forward(self.gen_x_fake_y) # reconstructs X (fake) from fake_Y images\n",
        "            gen_fake_x_fake_y = Gen_XY.gen_feed_forward(self.gen_y_fake_x) # reconstructs Y (fake) from fake_X images\n",
        "            \n",
        "            disc_fake_x = Disc_X.disc_feed_forward(self.gen_y_fake_x) # discriminates fake_x generated by gen_yx from real Y\n",
        "            disc_fake_y = Disc_Y.disc_feed_forward(self.gen_x_fake_y) # discriminates fake_y generated by gen_xy from real X\n",
        "            \n",
        "            disc_real_x = Disc_X.disc_feed_forward(self.X)\n",
        "            disc_real_y = Disc_Y.disc_feed_forward(self.Y)\n",
        "            \n",
        "            #####**********************************************\n",
        "            \n",
        "            # Discrimator loss\n",
        "            loss_disc_real_x = tf.reduce_mean(tf.squared_difference(disc_real_x, 1))\n",
        "            loss_disc_real_y = tf.reduce_mean(tf.squared_difference(disc_real_y, 1))\n",
        "\n",
        "            loss_disc_fake_x = tf.reduce_mean(tf.square(disc_fake_x))\n",
        "            loss_disc_fake_y = tf.reduce_mean(tf.square(disc_fake_y))\n",
        "\n",
        "            self.disc_X_total_loss = (loss_disc_real_x + loss_disc_fake_x) / 2\n",
        "            self.disc_Y_total_loss = (loss_disc_real_y + loss_disc_fake_y) / 2\n",
        "            \n",
        "            # Reconstruction loss\n",
        "            reconstruction_loss_x = tf.reduce_mean(tf.losses.mean_squared_error(self.X, gen_fake_y_fake_x))\n",
        "            reconstruction_loss_y = tf.reduce_mean(tf.losses.mean_squared_error(self.Y, gen_fake_x_fake_y))\n",
        "            total_reconstruction_loss = reconstruction_loss_x + reconstruction_loss_y\n",
        "            \n",
        "            # Generator loss\n",
        "            '''\n",
        "            Generator should be successful in fooling the discriminator. In other words, Generator should make the discriminator\n",
        "            believe that the generated images are real images. This can be done if the recommendation made by the Discriminator\n",
        "            is as close to 1 as possible. so, Generator would like to minimize ((Discriminator_X(Generator_YX(image))) - 1)^2\n",
        "            '''\n",
        "            gen_loss_fake_x = tf.reduce_mean(tf.squared_difference(disc_fake_x, 1))\n",
        "            gen_loss_fake_y = tf.reduce_mean(tf.squared_difference(disc_fake_y, 1))\n",
        "            \n",
        "            self.total_gen_XY_loss = (10 * total_reconstruction_loss) + gen_loss_fake_y\n",
        "            self.total_gen_YX_loss = (10 * total_reconstruction_loss) + gen_loss_fake_x\n",
        "#             gen_loss    = 10 * total_reconstruction_loss + (gen_loss_fake_x + gen_loss_fake_y)\n",
        "            disc_loss   = self.disc_X_total_loss + self.disc_Y_total_loss\n",
        "            \n",
        "            #####**********************************************\n",
        "            \n",
        "            \n",
        "            trainable_variables = tf.trainable_variables()\n",
        "            Gen_xy_variables = [var for var in trainable_variables if 'Gen_XY' in var.name]\n",
        "            Gen_yx_variables = [var for var in trainable_variables if 'Gen_YX' in var.name]\n",
        "            Disc_x_variables = [var for var in trainable_variables if 'Disc_X' in var.name]\n",
        "            Disc_y_variables = [var for var in trainable_variables if 'Disc_Y' in var.name]\n",
        "            \n",
        "            \n",
        "            \n",
        "            # Optimizers\n",
        "            self.Gen_XY_optim = tf.train.AdamOptimizer(learning_rate = self.lr, beta1 = 0.5).minimize(self.total_gen_XY_loss, var_list = Gen_xy_variables)\n",
        "            self.Gen_YX_optim = tf.train.AdamOptimizer(learning_rate = self.lr, beta1 = 0.5).minimize(self.total_gen_YX_loss, var_list = Gen_yx_variables)\n",
        "            self.Disc_X_optim = tf.train.AdamOptimizer(learning_rate = self.lr, beta1 = 0.5).minimize(self.disc_X_total_loss, var_list = Disc_x_variables)\n",
        "            self.Disc_Y_optim = tf.train.AdamOptimizer(learning_rate = self.lr, beta1 = 0.5).minimize(self.disc_Y_total_loss, var_list = Disc_y_variables)\n",
        "            \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "    ########################################################################\n",
        "    \n",
        "    def train_Disco_GAN(self):\n",
        "        count = 0\n",
        "        print('training the Disco GAN')\n",
        "        self.get_optimizer()\n",
        "        self.session = tf.Session()\n",
        "        self.session.run(tf.global_variables_initializer())\n",
        "        saver = tf.train.Saver()\n",
        "        \n",
        "        new_train_X_images_path = self.data_object.new_data_X_dir\n",
        "        new_train_Y_images_path = self.data_object.new_data_Y_dir\n",
        "        \n",
        "        num_of_train_X_images = len(self.data_object.train_X_images_names)\n",
        "        num_of_train_Y_images = len(self.data_object.train_Y_images_names)\n",
        "        num_of_train_images   = min(num_of_train_X_images, num_of_train_Y_images, 1060)\n",
        "        \n",
        "        \n",
        "        if self.to_restore:\n",
        "          \n",
        "          checkpoint_file = tf.train.latest_checkpoint(self.check_point_dir)\n",
        "          saver.restore(self.session, checkpoint_file)\n",
        "            \n",
        "        if not os.path.exists(self.check_point_dir):\n",
        "            os.makedirs(self.check_point_dir)\n",
        "        \n",
        "        for epoch in range(self.session.run(self.global_step), num_epochs):\n",
        "             \n",
        "            random.shuffle(self.data_object.train_X_images_names)\n",
        "            random.shuffle(self.data_object.train_Y_images_names)\n",
        "            count = 0\n",
        "            with tf.device('/gpu:0'):\n",
        "              for index in range(0, num_of_train_images, batch_size):\n",
        "#                   if count < 1:\n",
        "                    train_x = self.data_object.get_batch(index, self.data_object.train_X_images_names, \n",
        "                                                                        new_train_X_images_path, self.batch_size)\n",
        "                    train_y = self.data_object.get_batch(index, self.data_object.train_Y_images_names, \n",
        "                                                                       new_train_Y_images_path, self.batch_size)\n",
        "\n",
        "#                       if count == 0:\n",
        "#                           for arr in train_x:\n",
        "#                               (Image.fromarray(arr, 'RGB')).show()\n",
        "\n",
        "\n",
        "                     # training Gen_XY_optim to generate fake Y images from true X\n",
        "                    _, fake_Y = self.session.run([self.Gen_XY_optim, self.gen_x_fake_y], feed_dict = {self.X : train_x,\n",
        "                                                                                           self.Y : train_y})\n",
        "\n",
        "                    # training Gen_YX_optim to generate fake X images from true Y\n",
        "                    _, fake_X = self.session.run([self.Gen_YX_optim, self.gen_y_fake_x], feed_dict = {self.X : train_x,\n",
        "                                                                                               self.Y : train_y})\n",
        "                    _ = self.session.run([self.Disc_X_optim], feed_dict = {self.X : train_x,\n",
        "                                                                          self.Y : train_y})\n",
        "\n",
        "                    _ = self.session.run([self.Disc_Y_optim], feed_dict = {self.X : train_x,\n",
        "                                                                          self.Y : train_y})\n",
        "                    count = count + 1\n",
        "\n",
        "#                   else:\n",
        "#                       break\n",
        "\n",
        "              self.session.run(tf.assign(self.global_step, epoch + 1))\n",
        "              print('epoch ' + str(epoch) + ' samples ' + str(count))\n",
        "              if epoch % 100 == 0 and epoch >= 50:\n",
        "                saver.save(self.session,self.check_point_dir + 'DiscoGAN', global_step = epoch)\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    ########################################################################\n",
        "    \n",
        "    def create_directory(self, directory):\n",
        "        if not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "                \n",
        "    \n",
        "    \n",
        "    \n",
        "    ########################################################################\n",
        "    \n",
        "    def apply_transformtions_on_test_images(self):\n",
        "        print('testing the test images')\n",
        "        count = 0\n",
        "\n",
        "        \n",
        "        fake_X_path = self.output_dir + 'fake_X/'\n",
        "        fake_Y_path = self.output_dir + 'fake_Y/'\n",
        "        val_X_path  = self.output_dir + 'val_X/'\n",
        "        val_Y_path  = self.output_dir + 'val_Y/'\n",
        "        \n",
        "\n",
        "        self.create_directory(fake_X_path)\n",
        "        self.create_directory(fake_Y_path)\n",
        "        self.create_directory(val_X_path)\n",
        "        self.create_directory(val_Y_path)\n",
        "        \n",
        "        val_X_images_path = self.data_object.new_val_X_dir\n",
        "        val_Y_images_path = self.data_object.new_val_Y_dir\n",
        "\n",
        "        \n",
        "        val_X_images_names = self.data_object.val_X_images_names\n",
        "        val_Y_images_names = self.data_object.val_Y_images_names\n",
        "        \n",
        "        num_of_test_images = min(50, len(val_X_images_names), len(val_Y_images_names))\n",
        "        with tf.Session() as session:\n",
        "            meta_graph = tf.train.import_meta_graph(self.check_point_dir + 'DiscoGAN-2000.meta')\n",
        "            meta_graph.restore(session, tf.train.latest_checkpoint(self.check_point_dir))\n",
        "            \n",
        "            random.shuffle(val_X_images_names)\n",
        "            random.shuffle(val_Y_images_names)\n",
        "            \n",
        "            for index in range(0, num_of_test_images, self.batch_size):\n",
        "                val_x = self.data_object.get_batch(index, val_X_images_names, val_X_images_path, self.batch_size)\n",
        "                val_y = self.data_object.get_batch(index, val_Y_images_names, val_Y_images_path, self.batch_size)\n",
        "                \n",
        "                \n",
        "\n",
        "                fake_X, fake_Y = session.run([self.gen_y_fake_x, self.gen_x_fake_y], feed_dict = {self.X : val_x,\n",
        "                                                                                                   self.Y : val_y})\n",
        "                \n",
        "                for image_no in range(0, len(val_x)):\n",
        "                    cv2.imwrite((fake_X_path + str(count) + '_fx.png'), np.uint8(fake_X[image_no]))\n",
        "                    cv2.imwrite((fake_Y_path + str(count) + '_fy.png'), np.uint8(fake_Y[image_no]))\n",
        "                    cv2.imwrite((val_X_path + str(count) + '_tx.png'), np.uint8(val_x[image_no]))\n",
        "                    cv2.imwrite((val_Y_path + str(count) + '_ty.png'), np.uint8(val_y[image_no]))\n",
        "                    count = count + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8kupq5jjeAS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "    image_shape = [64, 64, 3]\n",
        "    batch_size = 10\n",
        "    learning_rate = 0.0002\n",
        "    num_epochs = 2001\n",
        "    to_restore = True\n",
        "    helper = helper_functions()\n",
        "    if not os.path.exists('/content/drive/My Drive/DiscoGAN'):\n",
        "        os.makedirs('/content/drive/My Drive/DiscoGAN')\n",
        "    output_dir  = '/content/drive/My Drive/DiscoGAN/test/'\n",
        "    \n",
        "    ### data object\n",
        "    data_X_path = '/content/drive/My Drive/horse2zebra/trainA/' # X\n",
        "    data_Y_path = '/content/drive/My Drive/horse2zebra/trainB/'   # Y\n",
        "    val_X_path = '/content/drive/My Drive/horse2zebra/testA/'\n",
        "    val_Y_path = '/content/drive/My Drive/horse2zebra/testB/'\n",
        "#     new_images_dir = '/Users/vijay/Downloads/DiscoGAN/new/'\n",
        "    data_object = Data_reading_and_processing(data_X_path, data_Y_path, val_X_path, val_Y_path)\n",
        "    disco_GAN = DISCO_GAN(image_shape, batch_size, learning_rate, num_epochs, to_restore, helper, output_dir, data_object)\n",
        "    disco_GAN.train_Disco_GAN()\n",
        "    disco_GAN.apply_transformtions_on_test_images()\n",
        "    \n",
        "    \n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qIa1Ge0jeAg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}